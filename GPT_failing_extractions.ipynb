{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a08JlmHHr6X1ByeQc14ek2lKAAEAMFJV",
      "authorship_tag": "ABX9TyO7pB1ZeZdthdh1FHqCLo7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richychn/midas/blob/main/GPT_failing_extractions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade python-dotenv"
      ],
      "metadata": {
        "id": "sn8xYMGtgHhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-index"
      ],
      "metadata": {
        "id": "fFoSPvpfX0ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "bVNQC-HkD6Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pathlib"
      ],
      "metadata": {
        "id": "LN8MZaU1YJ-8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-parse"
      ],
      "metadata": {
        "id": "0N91H9glWkDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install astrapy"
      ],
      "metadata": {
        "id": "MLOw08dTYCB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pyenv"
      ],
      "metadata": {
        "id": "gRbPq8Mmc23Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "#from openai import OpenAI\n",
        "#openai.organization = \"org-nWxH2RSTy0MZzrPL7efXAtPs\"\n",
        "\n",
        "import llama_index\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, download_loader\n",
        "\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.text_splitter import SentenceSplitter\n",
        "from llama_index.schema import TextNode\n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from llama_index.vector_stores import AstraDBVectorStore, VectorStoreQuery\n",
        "\n",
        "from google.colab import userdata\n",
        "from astrapy.db import AstraDB\n",
        "from pathlib import Path\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FLMnDe5UGxsd"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "openai.api_key = userdata.get('OPENAI_')\n",
        "\n",
        "\n",
        "db = AstraDB(\n",
        "  token=userdata.get('ASTRADB_TOKEN'),\n",
        "  api_endpoint=userdata.get(\"ASTRADB_API_ENDPOINT\"))\n",
        "\n",
        "print(f\"Connected to Astra DB: {db.get_collections()}\")\n",
        "\n",
        "vector_store = AstraDBVectorStore(\n",
        "            token=userdata.get('ASTRADB_TOKEN'),\n",
        "            api_endpoint=userdata.get(\"ASTRADB_API_ENDPOINT\"),\n",
        "            collection_name=\"midas_collection\",\n",
        "            embedding_dimension=384,\n",
        "            )\n",
        "\n",
        "parser = LlamaParse(\n",
        "    api_key=userdata.get(\"LLAMAPARSE_API\"),\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ciLLraifKc",
        "outputId": "7dfeabba-7b2b-41aa-d1f3-8812773d3feb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Astra DB: {'status': {'collections': ['midas_collection']}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-caf699423965>:11: UserWarning: Collection 'midas_collection' is detected as legacy and has indexing turned on for all fields. This implies stricter limitations on the amount of text each entry can store. Consider reindexing anew on a fresh collection to be able to store longer texts.\n",
            "  vector_store = AstraDBVectorStore(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_transcript_3 = '/content/drive/MyDrive/Midas - LlamaIndex Hackathon/Email PDFs/4__2024_02_03_08_10_pm.rtf'\n",
        "path_transcript_2 = '/content/drive/MyDrive/Midas - LlamaIndex Hackathon/Email PDFs/4__2024_02_03_07_45_pm.rtf'\n",
        "path_transcript_1 = '/content/drive/MyDrive/Midas - LlamaIndex Hackathon/Email PDFs/4__2024_02_03_07_28_pm.rtf'\n",
        "\n",
        "path_email = '/content/drive/MyDrive/Midas - LlamaIndex Hackathon/Email PDFs/LlamaJacket_Email.pdf'\n",
        "\n",
        "path_list = [path_transcript_1, path_transcript_2 ,path_transcript_3 ,path_email]"
      ],
      "metadata": {
        "id": "6niBuSVbWw9a"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list = SimpleDirectoryReader(input_files=path_list).load_data()"
      ],
      "metadata": {
        "id": "1PssTLa5p8zm"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_llama_jacket_doc = ''\n",
        "for i in range(len(docs_list)):\n",
        "  full_llama_jacket_doc = full_llama_jacket_doc + (docs_list[i].text)"
      ],
      "metadata": {
        "id": "e3q2cm0WjEiz"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmed_llama_jacket_doc = ''\n",
        "for i in range(2,8):\n",
        "  trimmed_llama_jacket_doc = trimmed_llama_jacket_doc + (docs_list[i].text)"
      ],
      "metadata": {
        "id": "R9fKFT5SlXaP"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Prompt = '''\n",
        "For each item that the buyer agreed to purchase, return the item name,\n",
        "item description, quantity purchased, unit price in dollars, unit discount\n",
        "in dollars, and final total price per item (quantity * (unit price - unit discount)).\n",
        "Each sku of the purchased item should be treated as a separate item.\n",
        "Output the result in json format following the example below.\n",
        "Each line item should have its own list under lineItems.\n",
        "Include all non quantity and price related details of each item under\n",
        "the description key of the dictionary\n",
        "{ \"orderDetails\":{ \"orderDate\": \"\", \"orderNum\": null, }, \"buyer\":{ \"companyName\": \"\", \"pointOfContact\": \"\", \"contactEmail\": \"\", \"contactPhone\": \"\", \"shippingAddress\": \"\", }, \"lineItems\":[{ \"itemNumber\": null, \"description\": \"\", \"quantity\": null, \"unitPrice\": null, \"discount\": null, “totalPrice”: null}]}\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "sGs54sfEN-Cm"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(context, given_prompt):\n",
        "  response = openai.chat.completions.create(\n",
        "      model = 'gpt-3.5-turbo'\n",
        "    , messages = [{\"role\":'system', 'content': context + ' ' + given_prompt}])\n",
        "  return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "6TpfbVnUdP27"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_response(full_llama_jacket_doc, Prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "HKsiJZe1Q_hR",
        "outputId": "31d71aa2-59a2-4b45-924c-9d7fc1d7cfff"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 6499 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-6d8cded765c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_llama_jacket_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-131-354d4b4fdb75>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(context, given_prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     , messages = [{\"role\":'system', 'content': context + ' ' + given_prompt}])\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 889\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 6499 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_response(trimmed_llama_jacket_doc, Prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iht6AW8cmZ9t",
        "outputId": "b0092c99-5b26-46d4-8308-ba71dd237280"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"orderDetails\": {\n",
            "    \"orderDate\": \"February 3, 2024\",\n",
            "    \"orderNum\": null\n",
            "  },\n",
            "  \"buyer\": {\n",
            "    \"companyName\": \"LlamaIndex\",\n",
            "    \"pointOfContact\": \"Kevin Smith\",\n",
            "    \"contactEmail\": \"kevinsmith@llama.com\",\n",
            "    \"contactPhone\": \"\",\n",
            "    \"shippingAddress\": \"2755 Augustine Dr, 8th Floor, Santa Clara, CA 95054\"\n",
            "  },\n",
            "  \"lineItems\": [\n",
            "    {\n",
            "      \"itemNumber\": null,\n",
            "      \"description\": \"LlamaIndex Custom Jacket - Design 3 (Cactus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_golf = '/content/drive/MyDrive/Midas - LlamaIndex Hackathon/Email PDFs/Old_DONOTUSE/Callaway Golf Bags.pdf'"
      ],
      "metadata": {
        "id": "Ne_snqvdrmBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list_golf = SimpleDirectoryReader(input_files=[path_golf]).load_data()\n",
        "full_golf_doc = ''\n",
        "for i in range(len(docs_list_golf)):\n",
        "  full_golf_doc = full_golf_doc + (docs_list_golf[i].text)\n"
      ],
      "metadata": {
        "id": "O3-TQxJZqvi-"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_response(full_golf_doc, Prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hd_eJI1rUDQ",
        "outputId": "5eaa2275-e18a-4c50-d87c-f01357a08a4d"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \"orderDetails\":{ \"orderDate\": \"Fri, Feb 3, 2024\", \"orderNum\": null, }, \"buyer\":{ \"companyName\": \"Golf-For-Fun\", \"pointOfContact\": \"Mohnish Shah\", \"contactEmail\": \"shahmohnish@gmail.com\", \"contactPhone\": \"\", \"shippingAddress\": \"\", }, \"lineItems\":[{ \"itemNumber\": null, \"description\": \"Callaway Premium Golf Bag\", \"quantity\": 200, \"unitPrice\": 75, \"discount\": 0, “totalPrice”: 15000}]}\n",
            "\n",
            "Explanation:\n",
            "- The order date is mentioned as February 3, 2024.\n",
            "- The buyer is Golf-For-Fun represented by Mohnish Shah.\n",
            "- The contact email mentioned is shahmohnish@gmail.com.\n",
            "- There is only one line item with the item description \"Callaway Premium Golf Bag\", a quantity of 200, a unit price of $75, no discount, and a total price of $15,000.\n"
          ]
        }
      ]
    }
  ]
}